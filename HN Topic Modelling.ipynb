{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] This trial license of GraphLab Create is assigned to ben@benmabey.com and will expire on August 05, 2015. Please contact trial@dato.com for licensing options or to request a free non-commercial license for personal or academic use.\n",
      "\n",
      "[INFO] Start server at: ipc:///tmp/graphlab_server-1473 - Server binary: /Users/bmabey/.virtualenvs/rbl-data/lib/python2.7/site-packages/graphlab/unity_server - Server log: /tmp/graphlab_server_1436666813.log\n",
      "[INFO] GraphLab Server Version: 1.4.1\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import graphlab as gl # this import will be assumed from now on\n",
    "import pandas as pd\n",
    "import funcy as fp\n",
    "import pyLDAvis\n",
    "import pyLDAvis.graphlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stories_sf = gl.load_sframe(\"hacker_news_stories.sframe\") #gl.load_sframe(\"http://s3.amazonaws.com/dato-datasets/hacker_news/stories_with_text.sframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "re_print = re.compile(r'[^' + string.printable + ']')\n",
    "re_contraction = re.compile(r'(\\w+)\\s+(t|ve|d|ll|m|re)\\b')\n",
    "\n",
    "def combine_and_clean(row):\n",
    "    cat = row[\"title\"] + ' ' + row[\"text\"]\n",
    "    ascii = re_print.sub(' ', cat)\n",
    "    fixed = re_contraction.sub(lambda m: \"'\".join(m.groups()), ascii)\n",
    "    return fixed\n",
    "\n",
    "stories_sf[\"text_title\"] = stories_sf.apply(combine_and_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stories_sf.save(\"hn_cleaned.sframe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = stories_sf# stories_sf.sample(0.4, seed=42)\n",
    "del sample[\"text\"]\n",
    "del sample[\"title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = sample.to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed, cpu_count\n",
    "\n",
    "def _series_chunks(s, n_jobs):\n",
    "    if n_jobs < 0:\n",
    "        # so, have n chunks if we are using all n cores/cpus = cpu_count() + 1 + n_jobs\n",
    "        n_jobs = cpu_count() + 1 + n_jobs\n",
    "    n = len(s)\n",
    "    n_chunks = int(n / n_jobs)\n",
    "    return (s.iloc[ilocs] for ilocs in fp.chunks(n_chunks, range(n)))\n",
    "\n",
    "def series_pmap(s, f, n_jobs=-1):\n",
    "    if n_jobs == 0:\n",
    "        return s.map(f)\n",
    "    return pd.concat(Parallel(n_jobs=n_jobs)(delayed(series_pmap)(sub_series, f, n_jobs=0) \\\n",
    "                                                 for sub_series in _series_chunks(s, n_jobs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from spacy.en import English\n",
    "nlp = English()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize(s):\n",
    "    tokens = nlp(unicode(s))\n",
    "    # graphlab doesn't handle namedtuples (or even tuples) so we have to use a list :(\n",
    "    return [[t.orth_.lower(), t.lemma_.lower(), t.pos, t.tag] for t in tokens if t.orth_.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_d(s):\n",
    "    tokens = nlp(unicode(s))\n",
    "    # graphlab doesn't handle namedtuples (or even tuples) so we have to use a list :(\n",
    "    return [[t.orth_, t.lemma_.lower(), t.pos_, t.tag_] for t in tokens if t.orth_.strip()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokens = series_pmap(df['text_title'], tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokens.to_pickle('hn_tokens.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import spacy.parts_of_speech as pos\n",
    "\n",
    "DEFAULT_POS = set([pos.NOUN, pos.VERB, pos.ADV, pos.ADJ])\n",
    "DEFAULT_STOPWORDS = gl.text_analytics.stopwords() | set(['pm','am', \"'re\", \"'ve\", \"n't\", 'thing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# chunks to train Phrases with\n",
    "def chunk(tokens, allowed_pos=DEFAULT_POS):\n",
    "    split = []\n",
    "    for token in tokens:\n",
    "        orth, lemma, pos, tag = token\n",
    "        if pos in allowed_pos:\n",
    "            split.append(lemma)\n",
    "        else:\n",
    "            # break detected!\n",
    "            if len(split) > 1:\n",
    "                yield split\n",
    "            if len(split) > 0:\n",
    "                split = []\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "def chunk_doc(tokens):\n",
    "    return list(chunk(tokens))\n",
    "    \n",
    "chunked_docs = tokens.map(chunk_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "contiguous_chunks = fp.cat(chunked_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# train chunker now\n",
    "bigram = Phrases(contiguous_chunks)\n",
    "trigram = Phrases(bigram[contiguous_chunks])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "tokenized_and_filtered_docs = chunked_docs.map(fp.cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_chunks(doc, min_len=3):\n",
    "    return [w for w in trigram[bigram[doc]] if len(w) >= min_len and w not in DEFAULT_STOPWORDS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "processed_docs = series_pmap(tokenized_and_filtered_docs, extract_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.corpora import Dictionary, MmCorpus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dictionary = Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dictionary.filter_extremes(no_below=5, no_above=0.3, keep_n=None)\n",
    "dictionary.compactify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def gensim2bows(corpus, dictionary):\n",
    "    return [{dictionary[id]: count for id, count in doc} for doc in corpus]\n",
    "gensim2gl = fp.compose(gl.SArray, gensim2bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bows = gensim2gl(corpus, dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_model100 = gl.topic_model.create(bows, num_topics=100, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "vis_data100 = pyLDAvis.graphlab.prepare(topic_model100, bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "topic_model75 = gl.topic_model.create(bows, num_topics=75, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "vis_data75 = pyLDAvis.graphlab.prepare(topic_model75, bows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Jul/2015 18:15:40] \"GET /d3.js HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Jul/2015 18:15:40] \"GET /LDAvis.js HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "stopping Server...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [11/Jul/2015 18:15:40] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [11/Jul/2015 18:15:40] \"GET /LDAvis.css HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: if you're in the IPython notebook, pyLDAvis.show() is not the best command\n",
      "      to use. Consider using pyLDAvis.display(), or pyLDAvis.enable_notebook().\n",
      "      See more information at http://pyLDAvis.github.io/quickstart.html .\n",
      "\n",
      "You must interrupt the kernel to end this command\n",
      "\n",
      "Serving to http://127.0.0.1:8889/    [Ctrl-C to exit]\n"
     ]
    }
   ],
   "source": [
    "pyLDAvis.show(vis_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
